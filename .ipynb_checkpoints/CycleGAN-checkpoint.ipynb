{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256,256*2)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CycleGANDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.files_a = glob.glob(os.path.join(folder,'A/*.jpg'))\n",
    "        self.files_b = glob.glob(os.path.join(folder,'B/*.jpg'))\n",
    "        self.images = []\n",
    "        for fn_a, fn_b in zip(self.files_a, self.files_b):\n",
    "            if len(self.images) % 100 == 0:\n",
    "                print(len(self.images))\n",
    "            try:\n",
    "                image_a = preprocess(Image.open(fn_a))\n",
    "                image_b = preprocess(Image.open(fn_b))\n",
    "            except IOError:\n",
    "                continue\n",
    "            self.images.append((image_a, image_b))\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.images[idx]\n",
    "        return out_data\n",
    "dataset = CycleGANDataset(\"/home/yanai-lab/terauchi-k/export/jupyter/notebook/pytorch-CycleGAN-and-pix2pix/datasets/datasets/apple2orange/train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DownSamp(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DownSamp, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=2,padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1,padding=1)\n",
    "        self.conv3 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1,padding=1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        return x\n",
    "class UpSamp(nn.Module):\n",
    "    def __init__(self, in_ch, cat_ch, out_ch):\n",
    "        super(UpSamp, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_ch,in_ch//2,kernel_size=4, stride=2,padding=1)\n",
    "        #forwardでcatする catするサイズはアップサンプル後\n",
    "        self.conv1 = nn.Conv2d(in_ch//2+cat_ch, out_ch, kernel_size=3, stride=1,padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1,padding=1)\n",
    "    def forward(self,x, cat):\n",
    "        x = self.deconv(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = torch.cat((x, cat),axis=1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        return x\n",
    "d = DownSamp(32,64)\n",
    "u = UpSamp(64,32,32)\n",
    "d(torch.zeros((1,32,64,64)))\n",
    "u(torch.zeros((1,64,32,32)),torch.zeros((1,32,64,64))).shape\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet,self).__init__()\n",
    "        self.in1 = nn.Conv2d(3, 32, kernel_size=7,stride=1,padding=3)\n",
    "        self.in2 = nn.Conv2d(32, 32, kernel_size=3,stride=1,padding=1)\n",
    "        self.down1 = DownSamp(32,64)\n",
    "        self.down2 = DownSamp(64,128)\n",
    "        self.down3 = DownSamp(128,256)\n",
    "        self.down4 = DownSamp(256,512)\n",
    "        self.up4 = UpSamp(512,256,256)\n",
    "        self.up3 = UpSamp(256,128,128)\n",
    "        self.up2 = UpSamp(128,64,64)\n",
    "        self.up1 = UpSamp(64,32,32)\n",
    "        self.out = nn.Conv2d(32,3, kernel_size=7, stride=1, padding=3)\n",
    "    def forward(self,x):\n",
    "        x = self.in1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x1 = self.in2(x)\n",
    "        x1 = F.leaky_relu(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up4(x5,x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up1(x, x1)\n",
    "        x = self.out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.spectral_norm import spectral_norm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, stride=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(32, 64, stride=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(64, 128, stride=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(128, 256, stride=2, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256,256,stride=2,kernel_size=3,padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256,512,stride=2,kernel_size=3,padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            #nn.Conv2d(1024,1024,stride=1,kernel_size=1,padding=0),\n",
    "            nn.Conv2d(512,1,stride=1,kernel_size=1,padding=0),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return (self.main(x).squeeze_(1))\n",
    "d = Discriminator()\n",
    "d(torch.zeros((1,6,256,256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "epoch = 3\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True)\n",
    "g_ab = UNet().to(device)\n",
    "g_ba = UNet().to(device)\n",
    "d_a = Discriminator().to(device)\n",
    "d_b = Discriminator().to(device)\n",
    "#zero_ = torch.zeros((batch_size,64)).to(device)\n",
    "#one_ = torch.ones((batch_size,64)).to(device)\n",
    "lossfunc = nn.MSELoss()\n",
    "g_opt = opt.SGD(params=g.parameters(),lr=1e-3, momentum=0.8)\n",
    "d_opt = opt.SGD(params=d.parameters(),lr=1e-2, momentum=0.8)\n",
    "#g_opt = opt.Adam(params=g.parameters())\n",
    "#d_opt = opt.Adam(params=d.parameters())\n",
    "itr = 0\n",
    "for ep in range(epoch):\n",
    "    for (img_a, img_b) in dataloader:\n",
    "        itr+=1\n",
    "        img_a = img_a.to(device)\n",
    "        img_b = img_b.to(device)\n",
    "        fake_b = g_ab(img_a)\n",
    "        fake_a = g_ba(img_b)\n",
    "        rec_a = g_ba(fake_b)\n",
    "        rec_b = g_ab(fake_a)\n",
    "        \n",
    "        l_g_aba_rec = torch.mean(torch.abs(img_a-rec_a))\n",
    "        l_g_bab_rec = torch.mean(torch.abs(img_b-rec_b))\n",
    "        l_g_ab_adv = \n",
    "        \n",
    "        g_loss = 1e-2*torch.mean(torch.abs(fake-real))\n",
    "        fake = torch.cat((img_a,fake),axis=1)\n",
    "        real = torch.cat((img_a,real),axis=1)\n",
    "        zero_ = torch.zeros((img_a.shape[0],16)).to(device)\n",
    "        one_ = torch.ones((img_a.shape[0],16)).to(device)\n",
    "        \n",
    "        g_loss += lossfunc(d(fake),one_)\n",
    "        #g_loss += -torch.mean(d(fake))\n",
    "        g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_opt.step()\n",
    "                \n",
    "\n",
    "\n",
    "        d_loss = lossfunc(d(real),one_) + lossfunc(d(fake.detach()),zero_)\n",
    "        #d_loss = -(torch.mean(torch.min(zero_,-one_+d(real)))+torch.mean(torch.min(zero_,-one_-d(fake.detach()))))\n",
    "        d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_opt.step()\n",
    "        if itr % 1000 == 1:\n",
    "            print(d_loss,g_loss)\n",
    "            print(\"a\")\n",
    "            plt.imshow(img_a.detach()[0].cpu().numpy().transpose(1,2,0))\n",
    "            plt.show()\n",
    "            print(\"b\")\n",
    "            plt.imshow(img_b.detach()[0].cpu().numpy().transpose(1,2,0))\n",
    "            plt.show()\n",
    "            print(\"a->b\")\n",
    "            plt.imshow(fake.detach()[0].squeeze()[3:].cpu().numpy().transpose(1,2,0))\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3_anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
